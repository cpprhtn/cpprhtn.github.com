<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Single Shot MultiBox Detector(SSD) - cpprhtn&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="dark"><meta name="application-name" content="cpprhtn&#039;s blog"><meta name="msapplication-TileImage" content="/img/cat.jpeg"><meta name="msapplication-TileColor" content="dark"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="cpprhtn&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="2022 Open Source Contribution Academy 의 Pytorch Hub 번역 팀의 일원으로 Single Shot MultiBox Detector(SSD) 논문에 대한 정리 및 예제 모델 학습을 해보았습니다. 논문 배경 상황 설명SSD는 Object Detection을 목표로 ECCV’ 16에 게제된 Paper 입니다. arxiv에는 2"><meta property="og:type" content="blog"><meta property="og:title" content="cpprhtn&#039;s blog"><meta property="og:url" content="https://cpprhtn.github.io/2022/07/13/Single-Shot-MultiBox-Detector-SSD/"><meta property="og:site_name" content="cpprhtn blog"><meta property="og:description" content="2022 Open Source Contribution Academy 의 Pytorch Hub 번역 팀의 일원으로 Single Shot MultiBox Detector(SSD) 논문에 대한 정리 및 예제 모델 학습을 해보았습니다. 논문 배경 상황 설명SSD는 Object Detection을 목표로 ECCV’ 16에 게제된 Paper 입니다. arxiv에는 2"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cpprhtn.github.io/img/logo1.png"><meta property="article:published_time" content="2022-07-13T06:15:52.000Z"><meta property="article:modified_time" content="2022-07-15T17:49:12.701Z"><meta property="article:author" content="cpprhtn"><meta property="article:tag" content="AI, paper"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/logo1.png"><meta property="twitter:creator" content="@cpprhtn"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cpprhtn.github.io/2022/07/13/Single-Shot-MultiBox-Detector-SSD/"},"headline":"Single Shot MultiBox Detector(SSD)","image":["https://cpprhtn.github.io/img/logo1.png"],"datePublished":"2022-07-13T06:15:52.000Z","dateModified":"2022-07-15T17:49:12.701Z","author":{"@type":"Person","name":"cpprhtn"},"publisher":{"@type":"Organization","name":"cpprhtn's blog","logo":{"@type":"ImageObject","url":"https://cpprhtn.github.io/img/logo1.png"}},"description":"2022 Open Source Contribution Academy 의 Pytorch Hub 번역 팀의 일원으로 Single Shot MultiBox Detector(SSD) 논문에 대한 정리 및 예제 모델 학습을 해보았습니다. 논문 배경 상황 설명SSD는 Object Detection을 목표로 ECCV’ 16에 게제된 Paper 입니다. arxiv에는 2"}</script><link rel="canonical" href="https://cpprhtn.github.io/2022/07/13/Single-Shot-MultiBox-Detector-SSD/"><link rel="icon" href="/img/cat.jpeg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo1.png" alt="cpprhtn&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/cpprhtn"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-07-13T06:15:52.000Z" title="2022. 7. 13. 오후 3:15:52">2022-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-07-15T17:49:12.701Z" title="2022. 7. 16. 오전 2:49:12">2022-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a></span><span class="level-item">21 minutes read (About 3212 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Single Shot MultiBox Detector(SSD)</h1><div class="content"><p><code>2022 Open Source Contribution Academy</code> 의 Pytorch Hub 번역 팀의 일원으로 <strong>Single Shot MultiBox Detector(SSD)</strong> 논문에 대한 정리 및 예제 모델 학습을 해보았습니다.</p>
<h2 id="논문-배경-상황-설명"><a href="#논문-배경-상황-설명" class="headerlink" title="논문 배경 상황 설명"></a>논문 배경 상황 설명</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.02325">SSD</a>는 Object Detection을 목표로 ECCV’ 16에 게제된 Paper 입니다.</p>
<p><code>arxiv</code>에는 2015/12월에 게제 되었으며, 근처에 나온 관련 논문으로는 <code>Fast R-CNN (ICCV&#39; 15)</code>, <code>Faster R-CNN (NIPS&#39; 15)</code>, <code>YOLO v1 (CVPR&#39; 16)</code>정도가 있습니다.</p>
<p>따라서 본 논문의 Introduction에서도 Faster R-CNN, YOLO(v1) <em>[당시에는 YOLO가 처음 나온 시기여서 version이 따로 붙어있지 않은 상태. 현재는 v7까지 나와있음.]</em> 과 비교하는 모습을 볼 수 있습니다.</p>
<p>당시의 시점에서 본 논문을 해석해봅시다.</p>
<span id="more"></span> 


<p>당시의 YOLO는 1-stage detector로 빠른 detection이 가능하였고(상대적으로 정확도가 낮음), R-CNN 기반의 2-stage detector 모델들은 selective search기반의 region proposals들을 추출하여 더 정확한 detection이 가능하다고 제안된 논문이였습니다(상대적으로 속도가 느림). 하지만 SSD는 기존에 제안된 논문들보다 더 빠르고 더 정확한 결과를 보여주었습니다.</p>
<h2 id="1-Model"><a href="#1-Model" class="headerlink" title="1. Model"></a>1. Model</h2><p><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/SSD.png" alt="SSD Model"></p>
<p>SSD는 base network와 auxiliary structure network로 이루어진 1-stage detector 입니다.<br>VGG-16 network를 base network로 사용하였고, auxiliary structure network로 Convolutional network를 사용하였습니다.<br>SSD는 VGG-16 network에서 많은 파라미터를 요구하는 fc레이어를 사용하지 않고 convolution layer로 대체하여 base network와 auxiliary structure network를 연결하였습니다.</p>
<h3 id="Multi-scale-feature-maps-for-detection"><a href="#Multi-scale-feature-maps-for-detection" class="headerlink" title="Multi-scale feature maps for detection"></a>Multi-scale feature maps for detection</h3><p><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/YOLO.png" alt="YOLO v1 Model"></p>
<p>YOLO의 7×7 single scale feature map과는 다르게 multi-scale feature maps을 사용하였습니다.<br>Base network의 마지막 부분에 convolution layer들을 추가하는데, 해당 layer들은 점진적으로 size가 감소하며, 이는 다양한 scale에 대해 뛰어난 detection 성능을 만들어냅니다.</p>
<ul>
<li>논문에서는 6종류의 scale을 가지는 feature maps를 사용하였습니다. (38×38, 19×19, 10×10, 5×5, 3×3, 1×1)</li>
</ul>
<h3 id="Convolutional-predictors-for-detection"><a href="#Convolutional-predictors-for-detection" class="headerlink" title="Convolutional predictors for detection"></a>Convolutional predictors for detection</h3><p><code>p</code>channels를 갖는 <code>m</code>×<code>n</code>의 feature map에 3×3×<code>p</code>크기의 convolution filter를 적용하였습니다.<br>Detection(Output)은 default bounding boxes의 category score와 box offsets을 측정합니다.<br>SSD는 Detection까지 convolution 연산을 하는 반면, YOLO는 fc 연산을 거치기 때문에 연산량과 속도 측면에서 효과적이였습니다.</p>
<h3 id="Default-boxes-and-aspect-ratios"><a href="#Default-boxes-and-aspect-ratios" class="headerlink" title="Default boxes and aspect ratios"></a>Default boxes and aspect ratios</h3><p>각각의 feature map cell마다 서로 다른 크기의 scale과 aspect ratio(종횡비)를 가지는 default box를 생성합니다. <em>[Default boxes는 Faster R-CNN의 anchor boxes와 비슷합니다. 하지만 6종류의 scale을 가진 feature map의 각각의 cell마다 default boxes를 생성한다는 점이 다릅니다.]</em><br>각각의 cell은 (<code>c</code>+4)<code>k</code> 개의 예측을 합니다. <code>c</code>는 class 수를, <code>k</code>는 default boxes의 개수를, 4는 4개의 offset를 의미합니다. 이를 <code>m</code>×<code>n</code>feature map에 적용하면 (<code>c</code>+4)<code>k</code> <code>m</code> <code>n</code> 개의 output이 출력됩니다.</p>
<h2 id="2-Training"><a href="#2-Training" class="headerlink" title="2. Training"></a>2. Training</h2><h3 id="Matching-strategy"><a href="#Matching-strategy" class="headerlink" title="Matching strategy"></a>Matching strategy</h3><p>Training을 진행하기위해 default boxes를 ground truth와 매칭합니다. 본 논문에서는 jaccard overlap(또는 Intersection Over Union<code>IOU</code>)값이 0.5 이상인 boxes들을 positive(Object가 있는/negative는 Object가 없는. 즉 배경을 의미) boxes로 사용합니다.</p>
<h3 id="Training-objective"><a href="#Training-objective" class="headerlink" title="Training objective"></a>Training objective</h3><p>Loss function은 localization loss(loc) 와 confidence loss(conf)의 가중 합 입니다.</p>
<p><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/loss.png" alt="Loss function"><br><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/loc.png" alt="loc"><br><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/conf.png" alt="conf"></p>
<ul>
<li>N: number of matched default boxes</li>
<li>x<sub>ij</sub><sup>p</sup> = {1, 0}: indicator for matching the i-th default box to the j-th ground truth box of category p / positive box = 1, negative box = 0</li>
<li>l: predicted box</li>
<li>g: ground truth box</li>
<li>d: default box</li>
<li>cx, cy: center x, y</li>
<li>w: width</li>
<li>h: height</li>
<li>a: 1</li>
</ul>
<h3 id="Choosing-scales-and-aspect-ratios-for-default-boxes"><a href="#Choosing-scales-and-aspect-ratios-for-default-boxes" class="headerlink" title="Choosing scales and aspect ratios for default boxes"></a>Choosing scales and aspect ratios for default boxes</h3><p><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/scale.png" alt="Correlation between feature map scale and default box scale"><br>위 그림의 (b)와 (c)를 보면 feature map의 크기가 작아질수록 더 큰 object detection이 가능함을 알 수 있습니다.  </p>
<p>본 논문에서는 이러한 default box의 scale을 다음과 같이 정의합니다.<br><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/default-boxes.png" alt="The scale of the default boxes for each feature map"></p>
<ul>
<li>m: Number of scale feature map (6)<br><code>s&lt;sub&gt;k&lt;/sub&gt;</code>값은 원본 이미지에 대한 비율을 의미하며, 해당 수식을 통해 다양한 크기의 default box를 생성할 수 있습니다.</li>
</ul>
<p>ex) 300×300 원본 이미지에 대하여 s = 0.1이고 aspect ratio가 1:1이면 default box의 size는 30×30이 됩니다.</p>
<h3 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h3><p>이미지를 생각해보면 object보다 background의 비중이 훨씬 높을겁니다. 이러한 상황을 반영하여 positive와 negative의 비율을 1:3으로 하여 데이터를 사용합니다.</p>
<h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h3><p>Robust한 모델을 얻기 위하여 data augmentation을 수행하였습니다.</p>
<h2 id="Result-from-the-Paper"><a href="#Result-from-the-Paper" class="headerlink" title="Result from the Paper"></a>Result from the Paper</h2><p>PASCAL VOC, COCO datasets에서 속도(FPS)와 정확도(mAP)가 가장 높은 모델이였습니다. (16년도 기준)<br><em>아래표에서 Fast YOLO는 2017에 제안된 논문으로 정확도를 포기하고 속도에만 중점을 둔 모델입니다.</em><br><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/result.png" alt="Results on Pascal VOC2007 test in 2016"></p>
<p>최근에 제안된 YOLO(single-scale)나 Swin(mulit-scale)에게 처참히 밀려있는 근황입니다.<br><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/paper.png" alt="paperswithcode의 Result from the Paper in 2022"></p>
<h2 id="SSD-모델-사용해보기"><a href="#SSD-모델-사용해보기" class="headerlink" title="SSD 모델 사용해보기"></a>SSD 모델 사용해보기</h2><p><a target="_blank" rel="noopener" href="https://pytorch.kr/hub/nvidia_deeplearningexamples_ssd/">Pytorch Hub - SSD</a>를 이용하여 쉽고 빠르게 모델을 불러와서 사용해볼 수 있었습니다.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">ssd_model = torch.hub.load(<span class="string">&#x27;NVIDIA/DeepLearningExamples:torchhub&#x27;</span>, <span class="string">&#x27;nvidia_ssd&#x27;</span>)</span><br><span class="line">utils = torch.hub.load(<span class="string">&#x27;NVIDIA/DeepLearningExamples:torchhub&#x27;</span>, <span class="string">&#x27;nvidia_ssd_processing_utils&#x27;</span>)</span><br></pre></td></tr></table></figure>
<details>
<summary>Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth</summary>
97.8M/97.8M [00:00<00:00, 143MB/s]
Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/ssd_pyt_ckpt_amp/versions/20.06.0/files/nvidia_ssdpyt_amp_200703.pt
Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub 

</details>

<h4 id="사전에-훈련된-SSD-모델과-유틸리티-파일들을-torch-hub를-통해-쉽게-불러올-수-있습니다"><a href="#사전에-훈련된-SSD-모델과-유틸리티-파일들을-torch-hub를-통해-쉽게-불러올-수-있습니다" class="headerlink" title="사전에 훈련된 SSD 모델과 유틸리티 파일들을 torch-hub를 통해 쉽게 불러올 수 있습니다."></a>사전에 훈련된 SSD 모델과 유틸리티 파일들을 torch-hub를 통해 쉽게 불러올 수 있습니다.</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssd_model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">ssd_model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<details>
<summary>SSD300(</summary>
  (feature_extractor): ResNet(
    (feature_extractor): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (5): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (6): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
  )
  (additional_blocks): ModuleList(
    (0): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (2): Sequential(
      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (3): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (4): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (loc): ModuleList(
    (0): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (conf): ModuleList(
    (0): Conv2d(1024, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): Conv2d(256, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
</details>

<h4 id="GPU를-사용할-수-있도록-지정해주고-불러온-모델을-확인해봅니다"><a href="#GPU를-사용할-수-있도록-지정해주고-불러온-모델을-확인해봅니다" class="headerlink" title="GPU를 사용할 수 있도록 지정해주고, 불러온 모델을 확인해봅니다."></a>GPU를 사용할 수 있도록 지정해주고, 불러온 모델을 확인해봅니다.</h4><p>앞에서 읽은 논문과는 다르게 base network가 VGG-16이 아닌 ResNet-50인것을 확인할 수 있습니다.  </p>
<p>논문에서 <code>We use the VGG-16 network as a base, but other networks should also produce good results.</code>라는 내용이 있었는데, base network를 성능이 더 좋은 ResNet을 사용한 것이라고 생각하고 넘어갔습니다.<br>(사실 코랩의 Model Description이나 파이토치 허브에서 ResNet를 사용한다고 설명하지만, 앞에서 논문 읽고 분석한걸 생색내려고 쓴 말입니다 &gt;&lt;)</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">uris = [</span><br><span class="line">    <span class="string">&#x27;http://images.cocodataset.org/val2017/000000397133.jpg&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://images.cocodataset.org/val2017/000000037777.jpg&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://images.cocodataset.org/val2017/000000252219.jpg&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h4 id="Object-Detection을-위한-테스트-이미지를-준비합니다"><a href="#Object-Detection을-위한-테스트-이미지를-준비합니다" class="headerlink" title="Object Detection을 위한 테스트 이미지를 준비합니다."></a>Object Detection을 위한 테스트 이미지를 준비합니다.</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = [utils.prepare_input(uri) <span class="keyword">for</span> uri <span class="keyword">in</span> uris]</span><br><span class="line">tensor = utils.prepare_tensor(inputs)</span><br></pre></td></tr></table></figure>
<h4 id="network의-input에-맞게-이미지를-포멧하고-텐서로-변환합니다"><a href="#network의-input에-맞게-이미지를-포멧하고-텐서로-변환합니다" class="headerlink" title="network의 input에 맞게 이미지를 포멧하고 텐서로 변환합니다."></a>network의 input에 맞게 이미지를 포멧하고 텐서로 변환합니다.</h4><p>SSD300 모델이므로 input shape는 (300, 300, 3).</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    detections_batch = ssd_model(tensor)</span><br></pre></td></tr></table></figure>
<h4 id="SSD모델을-통해-object-detection을-수행합니다"><a href="#SSD모델을-통해-object-detection을-수행합니다" class="headerlink" title="SSD모델을 통해 object detection을 수행합니다."></a>SSD모델을 통해 object detection을 수행합니다.</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results_per_input = utils.decode_results(detections_batch)</span><br><span class="line">best_results_per_input = [utils.pick_best(results, <span class="number">0.40</span>) <span class="keyword">for</span> results <span class="keyword">in</span> results_per_input]</span><br></pre></td></tr></table></figure>
<h4 id="신뢰도가-40-이상인-reasonable한-detections의-정보만-가져옵니다"><a href="#신뢰도가-40-이상인-reasonable한-detections의-정보만-가져옵니다" class="headerlink" title="신뢰도가 40% 이상인 reasonable한 detections의 정보만 가져옵니다."></a>신뢰도가 40% 이상인 reasonable한 detections의 정보만 가져옵니다.</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classes_to_labels = utils.get_coco_object_dictionary()</span><br></pre></td></tr></table></figure>
<h4 id="Class-ID를-object-name으로-매핑하기-위한-dictionary를-다운받습니다"><a href="#Class-ID를-object-name으로-매핑하기-위한-dictionary를-다운받습니다" class="headerlink" title="Class ID를 object name으로 매핑하기 위한 dictionary를 다운받습니다."></a>Class ID를 object name으로 매핑하기 위한 dictionary를 다운받습니다.</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(best_results_per_input)):</span><br><span class="line">    fig, ax = plt.subplots(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Show original, denormalized image...</span></span><br><span class="line">    image = inputs[image_idx] / <span class="number">2</span> + <span class="number">0.5</span></span><br><span class="line">    ax.imshow(image)</span><br><span class="line">    <span class="comment"># ...with detections</span></span><br><span class="line">    bboxes, classes, confidences = best_results_per_input[image_idx]</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bboxes)):</span><br><span class="line">        left, bot, right, top = bboxes[idx]</span><br><span class="line">        x, y, w, h = [val * <span class="number">300</span> <span class="keyword">for</span> val <span class="keyword">in</span> [left, bot, right - left, top - bot]]</span><br><span class="line">        rect = patches.Rectangle((x, y), w, h, linewidth=<span class="number">1</span>, edgecolor=<span class="string">&#x27;r&#x27;</span>, facecolor=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        ax.add_patch(rect)</span><br><span class="line">        ax.text(x, y, <span class="string">&quot;&#123;&#125; &#123;:.0f&#125;%&quot;</span>.<span class="built_in">format</span>(classes_to_labels[classes[idx] - <span class="number">1</span>], confidences[idx]*<span class="number">100</span>), bbox=<span class="built_in">dict</span>(facecolor=<span class="string">&#x27;white&#x27;</span>, alpha=<span class="number">0.5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<details>
<summary>결과 이미지 확인하기</summary>
<div markdown="1">

<p><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/output1.png" alt="confidence&gt;40%"></p>
<p><img src="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/output2.png" alt="confidence&gt;30%"></p>
</details>

</div><div class="article-licensing box"><div class="licensing-title"><p>Single Shot MultiBox Detector(SSD)</p><p><a href="https://cpprhtn.github.io/2022/07/13/Single-Shot-MultiBox-Detector-SSD/">https://cpprhtn.github.io/2022/07/13/Single-Shot-MultiBox-Detector-SSD/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>cpprhtn</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-07-13</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-07-16</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI-paper/">AI, paper</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/07/08/About-Block-Chain/"><span class="level-item">About Block Chain</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/cat.jpeg" alt="Junwon Lee"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Junwon Lee</p><p class="is-size-6 is-block">cpprhtn</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Korea, Busan</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">9</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/cpprhtn"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://instagram.com/cpp_rhtn"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/cpprhtn"><i class="fab fa-twitter"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://cpprhtn.notion.site/cpprhtn/34b2d52c37aa42b6b2d9ba5c39eb5472" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Notion</span></span><span class="level-right"><span class="level-item tag">cpprhtn.notion.site</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ROS/"><span class="level-start"><span class="level-item">ROS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hadoop/"><span class="level-start"><span class="level-item">hadoop</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/maven/"><span class="level-start"><span class="level-item">maven</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-13T06:15:52.000Z">2022-07-13</time></p><p class="title"><a href="/2022/07/13/Single-Shot-MultiBox-Detector-SSD/">Single Shot MultiBox Detector(SSD)</a></p><p class="categories"><a href="/categories/AI/">AI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-08T09:33:11.000Z">2022-07-08</time></p><p class="title"><a href="/2022/07/08/About-Block-Chain/">About Block Chain</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-01T09:57:11.000Z">2022-05-01</time></p><p class="title"><a href="/2022/05/01/About-Game-Programming/">About Game Programming</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-02-09T13:44:40.000Z">2022-02-09</time></p><p class="title"><a href="/2022/02/09/Mapreduce-Job-Process/">Mapreduce-Job-Process</a></p><p class="categories"><a href="/categories/hadoop/">hadoop</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-23T09:05:51.000Z">2022-01-23</time></p><p class="title"><a href="/2022/01/23/What-is-MapReduce/">맵리듀스란?</a></p><p class="categories"><a href="/categories/hadoop/">hadoop</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/06/"><span class="level-start"><span class="level-item">June 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI-TF/"><span class="tag">AI, TF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI-paper/"><span class="tag">AI, paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BlockChine/"><span class="tag">BlockChine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROS-Pixhawk-Jetson/"><span class="tag">ROS, Pixhawk, Jetson</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hadoop/"><span class="tag">hadoop</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hadoop-mapreduce/"><span class="tag">hadoop, mapreduce</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hadoop-maven/"><span class="tag">hadoop, maven</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/retrospect/"><span class="tag">retrospect</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo1.png" alt="cpprhtn&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 cpprhtn</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>