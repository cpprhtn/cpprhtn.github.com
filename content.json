{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Installing hadoop on MAC OS M1","text":"M1 맥북에서 하둡 환경을 구성하다보니 수많은 블로그를 참고하였고, 설명이 제각각이라 그대로 따라만 하면 설치할 수 있도록 정리해보았습니다. 1. Java JDK 설치Java JDK는 intel용을 설치하면 로제타2를 통해 돌아가므로 M1칩에서 네이티브로 돌아가는 Azul의 OpenJDK(download)를 설치하면 됩니다. java-8 lts / mac os / arm64 옵션이 맞는지 확인하고나서 .tar.gz 파일을 다운받고, 다운받은 파일을 더블클릭하여 실행시키면 설치가 진행됩니다. 이후에 터미널창을 열고 아래와 같이 Java path를 설정하시면 됩니다. 1234567cd ~vi ~/.zshrc export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/sourse ~/.zshrc java -version을 입력했을때 아래와 비슷한 결과가 나오면 제대로 설치된것입니다. 1234cpprhtn@cpprhtn-MacBookPro ~ % java -versionopenjdk version &quot;1.8.0_312&quot;OpenJDK Runtime Environment (Zulu 8.58.0.13-CA-macos-aarch64) (build 1.8.0_312-b07)OpenJDK 64-Bit Server VM (Zulu 8.58.0.13-CA-macos-aarch64) (build 25.312-b07, mixed mode) 2. Hadoop을 구축할 계정 생성하둡을 구축할 계정을 생성할 것입니다. 여러분이 현재 작업하던 환경과 같은 공간에서 하둡 환경을 구축하다가 예상치 못한 일이 발생했을때 복구하기 어려우므로 새로운 환경에서 환경을 구축하는 것입니다. 먼저 시스템 환경설정 -&gt; 사용자 및 그룹에 들어와 자물쇠를 풀어줍니다. 이후 ‘+’ 버튼을 눌러 사용자 계정을 하나 만들어줍니다. 여러분이 만든 사용자에 대하여 사용자를 이 컴퓨터의 관리자로 허용을 체크하여 관리자권한을 부여합니다. 관리자 권한을 주지 않으면 이후에 하둡 파일을 수정할때 접근권한이 없다고 뜹니다. 아직 재부팅에 들어가지 마세요 바로 이어서 시스템 환경설정 -&gt; 공유에 들어와 원격 로그인버튼을 체크해줍니다. 이제 시스템을 재부팅해 줄것인데, 여러분이 원래 작업하던 계정으로 다시 로그인하여 터미널을 오픈합니다. 새로 만들어준 계정으로 로그인 X 현재 사용하는 계정에서 새로 만든 계정으로 로그인합시다. 123su [새 계정][새 계정 password] 3. SSH 활성화로컬 호스트에 SSH로 연결할 수 있도록 보안 키를 만들고 암호없이 사용할 수 있도록 키를 복사하여 권한을 제한해줍니다. 12345678910111213141516# 새 계정의 home 디렉토리로 이동cd ~# 키생성ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa# 키복사cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# 권한 제한chmod 0600 ~/.ssh/id_rsa.pub# 작동여부 확인 후 종료ssh localhostexit 4. Apache Hadoop 설치하둡을 설치할텐데, 두가지 방법으로 설치가 가능합니다. 직접 다운받은후 경로 이동 Hadoop page에서 binary-aarch64 파일을 다운로드합니다. 다운받은 파일을 더블클릭하여 압축을 풀어줍니다. 압축이 풀린 파일을 새 계정의 home 디렉토리로 이동시켜줍니다. 1mv /Users/원래계정명/Downloads/hadoop-3.3.1 ~/ homebrew의 wget를 이용하여 간편설치 homebrew가 설치되어있는 사람은 아래 코드를 이용하여 바로 설치가 가능합니다. homebrew를 설치했고 잘 쓰고계시던 분들이 있을것입니다. 하지만 해당 포스트를 따라하면 새 계정을 만들어 작업하므로 새 게정의 home path의 .zshrc파일에서 homebrew path 설정이 안되어있을것입니다. 이런 경우에는 새 터미널 창을 하나켜서 본래 계정의 homebrew path를 카피하여 추가해주시기를 바랍니다.필자의 path는 아래와 같으며 여러분들도 이와 비슷한 경로일 것입니다. 1export PATH=/opt/homebrew/bin:$PATH 12345678# 다운로드wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz# 압축해제tar xzf hadoop-3.3.1.tar.gz# 압축파일 제거rm -rf hadoop-3.3.1.tar.gz 5. Hadoop settinglocal에서 하둡을 세팅하기 위해서는 몇가지 파일을 수정해주어야합니다. 먼저 .zshrc 파일을 수정해줍니다. 12345678910111213141516cd ~vi ~/.zshrc export HADOOP_HOME=/home/새 계정/hadoop-3.2.1 export HADOOP_INSTALL=$HADOOP_HOME export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin export HADOOP_OPTS&quot;-Djava.library.path=$HADOOP_HOME/lib/native&quot;source ~/.zshrc .zshrc에 추가되는 파일의 첫번째 export 라인의 path는 꼭 수정해줍시다. (새 계정명으로) hadoop-env.sh 파일 편집이후 hadoop-env.sh 파일에서 아래와 같이 주석되어있는부분을 주석을 해제하고 JAVA_HOME path를 세팅해줍니다. export JAVA_HOME=필자는 ~/.zshrc에서 이미 세팅해두었기때문에 해당 파일은 추가코드 없이 그냥 넘어갔습니다. 1234vi $HADOOP_HOME/etc/hadoop/hadoop-env.sh export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/ core-site.xml 파일 편집HDFS와 하둡의 핵심 속성을 정의해줍니다. 새 계정 이름으로 경로를 설정해줍시다. 1234567891011121314151617181920vi $HADOOP_HOME/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/Users/새 계정/hdfs/tmp/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://127.0.0.1:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;cd ~mkdir hdfscd hdfsmkdir tmp hdfs-site.xml 파일 편집hdfs-site.xml 파일은 노드 메타데이터, fsimage 파일 및 편집 로그 파일을 저장할 위치를 제어합니다. NameNode 및 DataNode 스토리지 디렉토리를 정의합니다. 새 계정 이름으로 경로를 설정해줍시다. value값에 설정된 숫자는 1~3으로 설정됩니다. 1은 로컬환경, 2는 가상 분할환경, 3은 완전 분할환경을 의미합니다. 이 포스트에서는 로컬환경을 구축하고 있으므로 value값은 1을 부여하였습니다. 123456789101112131415161718192021222324vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/Users/새 계정/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/Users/새 계정/hdfs/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;cd ~cd hdfsmkdir namenodemkdir datanode mapred-site.xml 파일 편집MapReduce 값을 정의할 수 있습니다. MapReduce 프레임워크 이름을 지정해주었습니다. 12345678vi $HADOOP_HOME/etc/hadoop/mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml 파일 편집여기에는 노드 관리자, 리소스 관리자, 컨테이너 및 애플리케이션 마스터에 대해 설정해줍니다. 코드가 길어도 수정할 부분은 없으니 복붙하여 쓰시면 될듯합니다. 123456789101112131415161718192021222324vi $HADOOP_HOME/etc/hadoop/yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.acl.enable&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 6. HDFS namenode formatHadoop 서비스를 처음 시작하기 전에 namenode를 포맷하는 것이 중요합니다. 123cd ~hdfs namenode -format 7. Hadoop 시작이전에 .zshrc에서 경로세팅을 했으므로 홈 디렉토리에서 아래와 같은 코드를 이용하여 하둡 서비스를 실행 및 종료 시킬 수 있습니다. 1234567891011cd ~# 모든 하둡 구성요소 실행start-all.sh# 실행 중인 모든 프로세스의 목록 확인jps# 모든 하둡 구성요소 종료stop-all.sh 우선은 start-all.sh을 실행시킨 후 10초정도 기다리면 하둡 구성요소들이 작동됩니다. 이후에 jps명령을 실행시키면 아래 사진과 같이 최소한 6개의 서비스가 표시되어야 합니다. (표시되는 순서나 넘버는 상관없음) 8. 브라우저에서 Hadoop UI에 액세스인터넷 브라우저를 열은 후에 localhost URL로 이동한 후 포트 번호를 사용하여 Hadoop UI에 접근할 수 있습니다. 9870 : NameNode 사용자 인터페이스 [전체 클러스터에 대한 포괄적인 개요를 제공] 9864 : DataNode 사용자 인터페이스 [브라우저에서 직접 개별 DataNode에 접근하는데 사용] 8088 : YARN Resource Manager [하둡 클러스터에서 실행 중인 모든 프로세스를 모니터링 할 수 있음] http://localhost:9870 http://localhost:9870 http://localhost:9870","link":"/2022/01/09/Installing-Hadoop-in-mac/"},{"title":"2021년 회고록","text":"2021년은 필자의 스무살!!! 대학교 새내기로 지내던 해입니다. 하지만 코로나의 여파로 대학교 생활은 물건너 갔습니다. 늦었다 생각하여 넘어가려고 했는데, 어떤 분이 용기를 주어 쓰게 되었습니다. 연구실 활동부경대학교 (21년 1월 ~ 현재)제 부족한 AI 커리어를 보고도 같이 AI연구 해보자고 교수님께 제안을 받게 되었습니다. 당시 고등학교 졸업장도 따지 못했던 시기인데 지금 생각해보면 엄청난 운이 뒤따른것 같습니다. 당시만 해도 연구실에서 연구를 한다는것은 되게 멋있는 일이라 생각했었고, 한번 해보자는 마음으로 연구실을 다니게 되었습니다. 가볍게 시작한 마음이였지만 어느덧 1년이 훌쩍 지나왔으며, 기대했던것 보다도 좋은 성능이 나와서 만족스러운 경험이였습니다. 해당 연구실을 다니며 논문을 읽는법이나 논문에서 제안된 AI모델을 구현해보고 적용해보면서 큰 폭으로 실력이 늘게된 계기가 되었습니다. 부산대학교 (21년 4월 ~ 현재)고2때부터 알고 지내던 쌤(이제는 교수님이라 불러야하지만 이미 쌤으로 define되어버림)이 AI 논문을 준비하게 되면서 연락이 왔었습니다. 처음에는 논문에 넣을 연구를 도와주려고 연구실을 들락날락 했었는데, 매일 가다보니 연구원분들과도 친해지고, 교수님한테는 쌤 제자라고 소개받게되면서 자연스럽게 연구실을 다니게 되었습니다. 쌤의 연구를 도와주면서 frequency와 CV(Computer Version)에 대한 깊이있는 공부를 할 수 있었으며 연구실에 남아돌던 Pixhawk4 STM보드가 있어서 드론+CV 연구도 해볼 수 있었습니다. 또한 해당 연구실이 포닥 위주로 구성되어있어서 연구원분들께 많은 것을 보고 배울 수 있는 좋은 경험이였습니다. 한국교원대학교 (21년 1월)알던 형의 소개로 박사학위논문 연구에 도움이 필요하다 하여 어쩌다보니 연구에 참여하게 되었습니다. 짧은 기간동안 질문에 대한 많은 답변과 피드백을 남기며 끝이 났으며, 프로그래밍 공부방법에 대한 성찰을 가지게 된 계기가 되었습니다. 최근에 무사히 졸업하셨다는 연락을 받았으며, 참여한 논문은 아직 출판전이라 따로 우편으로 받아서 읽어도 보았습니다. 내가 낸 의견과 답변, 피드백이 논문에 반영된 모습은.. 정말 신기했습니다. 강의 활동회고를 쓰면서 강의활동도 정리해보았는데 생각보다 많더군요. 먼저 제 첫 강의는 부경대학교 대학원생을 상대로 한 인공지능 강의입니다. 인생 첫 강의여서 많이 떨렸었는데 강의를 막상 시작하니까 떨림은 어디가고 재미있어서 신나게 강의했던 기억이 납니다. 개인이 진행해보는 강의라서 모든 강의자료를 직접 다 만들어야 했던 경험은 당시에는 악몽이였고, 나중에서는 신의 한수가 됩니다. 4개월간 꾸준히 강의를 진행했으며 강의자료를 만들면서, 혹은 강의후에 들어오는 다양한 질문들에 대해 답변해주면서 AI의 기초를 더 탄탄히 다질 수 있는 기회가 되었습니다. 두 번째, 세 번째, 네 번째는 위에서 언급한 “쌤”이 하는 강의에서 보조강사로 활동하게 되었습니다. C언어와 OPENCV 강의(2번)의 보조강사로 있으면서 “쌤”이 어떻게 강의를 진행하는지, 말 안듣는 잼민이(고등학생도 잼민이겠죠?)는 어떻게 하는지 등과 같은 것들을 볼 수 있었습니다. OPENCV 강의가 기억에 남는데, 인공지능이 갑자기 뜨면서 OPENCV 강의를 잡아온거라 강의자료를.. A에서 Z까지 다 만들었습니다. Jupyter Notebook에 데이터, 이미지, 코드, 코드설명을 꼼꼼히 적고 검수하고 했다는..ㅠㅠ 마지막으로는 과학고등학교 강의를 하게 되었습니다. 강의 하기로 확정이 되자, “Tensorflow(인공지능 프레임워크) 여름캠프”라는 이름하에 강의가 진행되었고, 첫 강의였던 부경대학교 강의때 열심히 만들어 둔 자료를 기반으로 딱딱한 내용들은 빼고, 학생들이 흥미로워 할 내용들은 추가하여 강의를 진행하게 되었습니다. 생각보다 학생들이 잘 따라와주어서 재미있게(저혼자만 그렇게 생각할수 있음) 강의를 진행하였고, 강의후에 추가로 진행한 AI 팀프로젝트는 생각보다도 더 수준높은 결과물을 만들어와서 역시 과고는 괴물만 사는구나 라는 기억이 남던 경험이 되었습니다. 2022년 목표꼭 해야할 목표 몇가지만 세우고 글을 마무리 지으려고 합니다. (스스로의 다짐이려나?) Data science 공부하기 hadoop spark hive more AI 동아리 만들기 (신설학과라 AI관련 동아리가 없더라구요) 연구 잘 마무리하기 책 무사히 출판하기 스타트업(6월쯤 AI 개발팀장으로 들어갈예정) 잘 운영(?)하기 끝으로스무살, 첫 회고록을 써 보았습니다. 글을 잘 쓰는 편이 아니라 이 상태로 올려도 될지 고민이 들고, 이렇게 글을 써도 되는거 맞냐는 의문이 들지만 “어차피 볼사람도 없을텐데 고민하면 뭐하냐”라고 세뇌하며 글을 마무리합니다.","link":"/2022/01/11/2021-retrospect/"},{"title":"HDFS란?","text":"HDFS(Hadoop Distributed File System)는 수십 테라바이트 이상의 대용량 파일을 분산된 서버에 저장하고, 많은 클라이언트가 저장된 데이터를 빠르게 처리할 수 있게 설계된 파일 시스템입니다. 1. HDFS란?기존에도 DAS, NAS, SAN과 같은 대용량 파일 시스템이 있었으나 HDFS와 기존 대용량 파일 시스템의 가장 큰 차이점은 저사양 서버를 이용해 스토리지를 구성할 수 있다는 것입니다. HDFS를 사용하면 고사양 서버에 비해 매우 저렴한 저사양 서버 수십, 수백대를 묶어서 하나의 스토리지처럼 사용할 수 있으며 물리적으로는 분산된 서버의 로컬 디스크에 저장되어있지만 HDFS에서 제공하는 API를 이용하여 파일의 읽기 및 저장을 마치 한 서버에서 작업하듯이 구성할 수 있습니다. 그렇다고 HDFS가 기존 대용량 파일 시스템을 완전히 대체하는 것은 아닙니다. 고성능, 고가용성이 필요한 경우에는 SAN을, 안정적인 파일 저장이 필요한 경우에는 NAS를 사용합니다. 또한 트랜잭션이 중요한 경우에도 HDFS가 적합하지 않으며, 대규모 데이터를 저장하거나, 배치로 처리를 하는 경우에 HDFS를 사용하면 됩니다. HDFS를 만든이유HDFS는 네 가지 목표를 가지고 설계되었습니다. 장애 복구 서버에는 다양한 장애가 발생할 수 있습니다. 이러한 서버를 수십, 수백대를 묶어서 구축한 분산 서버에는 당연히 장애가 생길 확률이 높아집니다. HDFS는 이러한 장애를 빠른 시간에 감지하고 대처할 수 있게 설계되어 있습니다. 스트리밍 방식의 데이터 접근 HDFS는 클라이언트의 요청을 빠른 시간 내에 처리하는 것보다는 동일한 시간 내에 더 많은 데이터를 처리하는 것을 목표로 합니다. 이를 위해 HDFS는 랜덤 접근 방식 대신 스트리밍 방식으로 데이터에 접근하도록 설계되어 있습니다. 대용량 데이터 저장 HDFS는 파일 하나의 크기가 테라바이트 이상의 크기로도 저장될 수 있게 설계되었습니다. 따라서 높은 데이터 전송 대역폭과, 하나의 클러스터에서 수백 대의 노드를 지원 할 수 있습니다. 또한 하나의 인스턴스에서는 수백만개 이상의 파일을 지원합니다. 데이터 무결성 HDFS에서는 한번 저장한 데이터는 더이상 수정할 수 없고, 읽기만 가능하게 하여 무결성을 유지했습니다. 하지만 데이터 수정이 불가능하다는 것은 ㅋ 하둡 초기에서부터 검토되어 왔고, 하둡 2.0 버전부터는 HDFS에 저장된 파일에 append 기능이 추가되었습니다. HDFS 아키텍처 블록 구조 파일 시스템 HDFS는 블록 구조의 파일 시스템입니다. HDFS에 저장하는 파일은 특정 크기의 블록으로 나눠져 분산된 서버에 저장합니다. 하둡의 버전에 따라서 블록 크기의 defalt 세팅값이 달라지는데, 하둡 2.0 이상버전에서는 블록당 128MB의 크기를 가지고 있습니다. 따라서 예를 들어 200MB 파일이 있다면 128MB 블록 2개를 사용하게 되며, 블록 하나는 128MB를 전부 사용하게 되며, 다른 블록하나는 128MB를 다 차지하는 것이 아니라 78MB를 차지하게 되는 형태입니다. 그렇기 때문에 데이터 크기에 상관없이 저장할 수 있습니다. 또한 HDFS에서는 기본적으로 3개의 블록 복제본을 저장하게 됩니다. 이는 특정 서버의 하드디스크에 오류가 생기더라도 복제된 블록을 사용하여 데이터를 조회할 수 있습니다. 블록의 크기와 복제본의 수는 하둡 환경설정 파일에서 변경할 수 있습니다. 네임노드와 데이터노드 네임노드네임노드는 다음과 같은 기능을 합니다. 메타데이터 관리 네임노드는 파일 시스템을 유지하기 위한 메타데이터를 관리합니다. 메타데이터는 파일시스템 이미지와 파일에 다한 블록 매핑 정보로 구성됩니다. 데이터노드 모니터링 데이터노드가 네임노드에게 3초마다 heartbeat 메시지를 전송합니다. heartbeat는 데이터노드 상태 정보와 block-report로 구성됩니다. 네임노드는 heartbeat를 이용하여 데이터노드의 실행 상태와 용량을 모니터링합니다. 일정 시간 내에 heartbeat 메시지가 들어오지않는 데이터노드가 있을 경우 장애가 발생한 서버로 판단합니다. 블록 관리 데이터노드에 장애가 발생시 해당 데이터노드의 블록을 새로운 데이터노드로 복제합니다. 용량이 부족한 데이터노드가 있다면 여유있는 데이터노드로 블록을 이동시킵니다. 블록의 복제본 수 또한 관리합니다. 클라이언트 요청 접수 클라이언트가 HDFS에 접근하려면 반드시 네임노드에 접속해야합니다. HDFS에 파일을 저장하는 경우 기존 파일의 저장 여부와 권한 확인 절차를 거쳐서 저장을 승인합니다. 또한 파일을 조회하는 경우에는 블록의 위치 정보를 반환합니다. 데이터노드데이터노드는 클라이언트가 HDFS에 저장하는 파일을 로컬 디스크에 유지합니다. 이때 로컬 디스크에 저장되는 파일은 두 종류로 구성됩니다.첫 번째 파일은 실제 데이터가 저장되어 있는 로우 데이터이며, 두 번째 파일은 체크섬이나 파일 생성 일자와 같은 메타데이터가 설정되어 있는 파일입니다. 보조네임노드 네임노드 네임노드는 메타데이터를 메모리에서 처리합니다. 하지만 메모리에만 데이터를 유지할 경우 서버가 재부팅되면 모든 메타데이터를 유실할 수 있습니다. HDFS에서는 이러한 문제점을 해결하기 위해 editslog와 fsimage라는 두 개의 파일을 생성합니다. editslog: HDFS의 모든 변경 이력을 저장한 파일.fsimage: 메모리에 저장된 메타데이터의 파일 시스템 이미지를 저장한 파일. 네임노드 작동시 네임노드가 구동되면 로컬에 저장된 fsimage와 editslog를 조회 메모리에 fsimage를 로딩해 파일시스템 이미지를 생성 메모리에 로딩된 파일 시스템 이미지에 editslogdp 기록된 변경 이력을 적용 메모리에 로딩된 파일 시스템 이미지를 이용해 fsimage 파일을 갱신 editslog 초기화 데이터노드가 전송한 플록리포트를 메모리에 로딩된 파일 시스템 이미지에 적용 보조네임노드 editslog는 별도의 크기제한이 없기때문에 무한대로 커질 수 있기 때문에 editslog의 크기가 계속해서 커지면 위 단계중 3번 단계를 진행할 때 많은 시간이 소요될 것입니다. 이러한 문제점을 해결하기 위해 HDFS는 보조네임노드라는 노드를 제공합니다. 보조네임노드는 주기적으로 네임노드의 fsimage를 갱신하는 역할을 하며, 이러한 작업을 체크포인트라고 합니다. 보조네임노드는 다음과 같은 체크포인팅 단계를 거쳐 네임노드의 fsimage를 갱신해줍니다. 이렇게 체크포인팅이 완료되면 네임노드의 fsimage는 최신 내역으로 갱신되며, editslog의 크기도 축소됩니다. 기본적으로 한시간마다 체크포인팅이 발생하며, 하둡 환경설정 파일에서 변경할 수 있습니다. 보조네임노드도 중요하다 보조네임노드를 중요하게 생각하지 않거나 보조네임노드를 네임노드의 백업 노드라고 생각할 수도 있습니다. 하지만 앞에서 설명했듯이 보조네임노드는 네임노드의 (fsimage 갱신, editslog 축소) 시켜주는 역할을 하므로 백업과는 관련이 없습니다. 또한 보조네임노드가 다운되어있어도 네임노드의 동작에는 전혀 문제가 없기때문에 보조네임노드를 방치해둘지도 모릅니다. 하지만 네임노드를 재구동하는 상황이 생겼을때, editslog의 크기가 너무 커서 네임노드의 메모리에 로딩되지 못하는 상황이 발생할 수 있습니다. 따라서 이러한 장애를 사전에 방지하기 위해서는 주기적으로 보조네임노드를 관리해주어야 합니다. HDFS Architecture","link":"/2022/01/11/About-HDFS/"},{"title":"How to install Maven on M1","text":"&lt;&lt;시작하세요 하둡 프로그래밍&gt;&gt;이라는 책을 통해 하둡을 공부하면서, 이 책의 예제파일을 다운받고 실행을 시켜보려고 했습니다. Eclipse나 InteliJ를 사용하는 유저라면 간단한 세팅을 통하여 mvn 프로젝트를 실행시킬 수 있지만, 저는 터미널에서 package사용과 mvn명렁어를 사용하기 위해서 직접 M1 mac에 설치하게 되었습니다. Apache Maven 다운Apache Maven 주소에서 최신 버전의 .tar.gz파일을 내려받습니다. tar -vxf 명령어로 압축을 해제하거나, 다운받은 파일을 더블클릭하여 압축을 해제해줍니다. PATH 설정원하는 위치로 해당 디렉토리를 옮겨준 후 Path를 설정해줍니다. 1234567vi ~/.zshrc # set maven path export M2_HOME=/Users/bigdata/apache-maven-3.8.4 #maven 주소 / 사람마다 다르므로 변경해줘야함 export PATH=$PATH:$M2_HOME/binsource ~/.zshrc 설치 확인mvn -version 명령어를 통해 메이븐이 제데로 설치되었는지 확인한다.","link":"/2022/01/17/How-to-install-Maven-on-M1/"},{"title":"Hadoop HDFS fs Command Error","text":"하둡 HDFS 명령어란하둡은 사용자가 HDFS를 쉽게 제어할 수 있게 shell 명령어를 제공합니다. 이 shell 명령어는 fs(FileSystem Shell)라고 하며 아래와 같이 사용하게 됩니다. 1234./bin/hadoop fs -[cmd] [args]# PATH setting이 된 경우hadoop fs -[cmd] [args] 에러가 뜨는 경우 (WARNING은 해당사항 X)일단 당연하게도 에러가 발생했을때, 해당 에러명을 복사해서 구글링을 해보는 방법이 첫번째 입니다. 그래도 해결되지 않는 경우에 최후의 수단으로 아래와 같은 방법을 통하여 namenode를 format해버리는 방법이 있습니다. 1234567891011# hadoop를 실행중이라면 종료stop-all.sh# namenode formathadoop namenode -format# 하둡 실행start-all.sh# 명령어 작동 확인hadoop fs -[명령어] [파라미터]","link":"/2022/01/18/Hadoop-fs-ERROR/"},{"title":"하둡 HDFS 입출력 따라해보기","text":"Maven project에 입/출력 예제 코드 생성우선 maven project 디렉토리로 이동해줍니다. maven project를 만드는 방법을 모르는 분은 Maven으로 Java 프로젝트 시작하기라는 본문의 포스트를 참고해주세요. ./maven project path/src/main/java/package name/ 경로까지 들어오게되면 App.java라는 java 파일이 있는 위치까지 오게됩니다. (경로는 사람마다 다릅니다.) 필자는 프로젝트 명을 study-hadoop라고 지었고 package는 cpprhtn, 현재 경로는 /Users/bigdata/study-hadoop/src/main/java/cpprhtn 입니다. 이후에 아래와 같은 코드를 짜줍니다. HDFS_IO.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344package cpprhtn; # 각자의 패키지명에 맞추어 수정import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class HDFS_IO { # 생성한 파일명과 클래스명이 완벽하게 일치해야 함 public static void main(String[] args) { // 입력 파라미터 확인 if (args.length != 2) { System.err.println(&quot;Usage: HDFS_IO &lt;filename&gt; &lt;contents&gt;&quot;); System.exit(2); } try { // 파일 시스템 제어 객체 생성 Configuration conf = new Configuration(); FileSystem hdfs = FileSystem.get(conf); // 경로 체크 Path path = new Path(args[0]); if (hdfs.exists(path)) { hdfs.delete(path, true); } // 파일 저장 FSDataOutputStream outStream = hdfs.create(path); outStream.writeUTF(args[1]); outStream.close(); // 파일 출력 FSDataInputStream inputStream = hdfs.open(path); String inputString = inputStream.readUTF(); inputStream.close(); System.out.println(&quot;Input Data:&quot; + inputString); } catch (Exception e) { e.printStackTrace(); } }} Java API를 이용하여 HDFS에 파일을 생성하는 예제입니다. 파일이 저장될 경로와 파일에 생성할 문자열을 입력받아 지정된 경로에 .txt파일을 생성하는 코드입니다. 코드를 입력하여 저장하였다면 maven project path 시작지점으로 돌아가 mvn package를 해주면 됩니다. (필자는 study-hadoop 디렉토리가 되겠습니다.) 하지만 아직 Hadoop API를 이용할 수 없는 상황이라 에러 문구가 뜰 것입니다. 이는 pom.xml에 다음과 같은 Maven 종속성이 포함되어야 합니다. 1234567891011121314151617181920# 위치를 잘 찾아서 추가할 것&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 추가한 후에 다시 mvn package를 사용하여 빌드하면 BUILD SUCCESS라는 문구가 뜰 것입니다. 이제 하둡에서 돌릴 jar 파일은 maven project path/target에 생성될 것입니다. Hadoop에서 입/출력 클래스 실행이제 하둡에서 실행시키기 위해 하둡 서비스를 실행시켜줍니다. 1start-all.sh 이후 하둡 HDFS에 입/출력이 잘 동작하는지 확인해봅시다. 1hadoop jar study-hadoop-1.0-SNAPSHOT.jar cpprhtn.HDFS_IO input.txt Hello,Hadoop_and_HDFS 위 코드를 실행시키면 study-hadoop-1.0-SNAPSHOT.jar 파일의 cpprhtn package의 HDFS_IO 클래스를 사용한다는 것이며, 입력받은 파일명과 문자열을 HDFS내에 만들어줍니다. 1hadoop fs -ls /user/bigdata/ bigdata 대신에 여러분의 User명을 넣게되면 아래와같은 내용이 출력됩니다. 123bigdata@cpprhtn-MacBookPro target % hadoop fs -ls /user/bigdataFound 1 items-rw-r--r-- 1 bigdata supergroup 14 2022-01-22 20:18 /user/bigdata/input.txt 하둡 HDFS에 제대로 생성된 것을 볼 수 있으며, 만들어진 input.txt는 다음과 같이 확인할 수 있습니다. 12bigdata@cpprhtn-MacBookPro target % hadoop fs -cat /user/bigdata/input.txtHello,Hadoop_and_HDFS","link":"/2022/01/19/Hadoop-print-Hello-HDFS/"},{"title":"Maven으로 Java 프로젝트 시작하기","text":"hadoop또한 java를 사용한다. 따라서 java 프로젝트를 만들어서 사용해야합니다. Eclipse나 InteliJ를 사용하는 것이 아닌 mac의 터미널을 사용하여 java를 빌드하고 jar 파일을 생성하여 사용할 예정입니다. Maven을 설치하였다면 아래 순서를 따라 maven project를 만들어 볼 수 있습니다. 1. 프로젝트를 만들 디렉토리로 이동.12su bigdatacd ~ 2. maven project 생성시작123456mvn archetype:generate--- Scanning for projects (처음 한번만 오래걸림 / 이후에는 버전체크후 빠르게 넘어감) ------ 생략 --- 3. 세부 세팅이러한 글에서 동작이 자동으로 멈출것입니다. 3-1. 템플릿 선택템플릿을 정하는 부분입니다.default로는 quickstart 템플릿이 사용됩니다.그냥 아무입력 없이 Enter를 눌러줍시다. 1Choose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): 1854: 3-2. 템플릿 버전 선택여기는 템플릿 버전을 선택하는 부분입니다.Enter를 눌러주면 default값인 가장 최신버전이 선택됩니다. 12345678910Choose org.apache.maven.archetypes:maven-archetype-quickstart version: 1: 1.0-alpha-12: 1.0-alpha-23: 1.0-alpha-34: 1.0-alpha-45: 1.06: 1.17: 1.38: 1.4Choose a number: 8: 3-3. 그룹 ID그룹 ID를 물어봅니다.해당 프로젝트를 생성하는 개인 또는 단체를 의미하는 ID입니다. 1Define value for property 'groupId': com.cpprhtn 3-4. 아티팩트 ID아티팩트 ID는 생성할 프로젝트의 ID(이름)입니다.저는 하둡공부용 프로젝트를 만들것이므로 아래와 같이 적었습니다. 1Define value for property 'artifactId': study-hadoop 3-5. 버전프로그램의 버전을 지정합니다.default값인 1.0-SNAPSHOT를 그대로 사용할 예정입니다. 1Define value for property 'version' 1.0-SNAPSHOT: 3-6. 패키지프로그램을 배치할 패키지를 지정합니다.defalut값으로는 그룹 ID가 설정되어있습니다. 1Define value for property 'package' com.cpprhtn: : cpprhtn 3-7. 입력확인앞에서 채운 내용이 맞는지 마지막으로 한번 확인해줍니다.별 이상이 없을경우 Enter / 문제가 있을경우 (n/N)을 입력해주면 다시 처음부터 설정할 수 있습니다. 123456Confirm properties configuration:groupId: com.cpprhtnartifactId: study-hadoopversion: 1.0-SNAPSHOTpackage: cpprhtn Y: 4. Maven Project 생성완료아래와 같이 BUILD SUCCESS가 뜨면 정상적으로 생성이 완료된것입니다. 12345678910111213141516171819[INFO] ----------------------------------------------------------------------------[INFO] Using following parameters for creating project from Archetype: maven-archetype-quickstart:1.4[INFO] ----------------------------------------------------------------------------[INFO] Parameter: groupId, Value: com.cpprhtn[INFO] Parameter: artifactId, Value: study-hadoop[INFO] Parameter: version, Value: 1.0-SNAPSHOT[INFO] Parameter: package, Value: cpprhtn[INFO] Parameter: packageInPathFormat, Value: cpprhtn[INFO] Parameter: package, Value: cpprhtn[INFO] Parameter: version, Value: 1.0-SNAPSHOT[INFO] Parameter: groupId, Value: com.cpprhtn[INFO] Parameter: artifactId, Value: study-hadoop[INFO] Project created from Archetype in dir: /Users/bigdata/study-hadoop[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 25.657 s[INFO] Finished at: 2022-01-21T17:21:49+09:00[INFO] ------------------------------------------------------------------------ 5. 프로젝트 구성.├── pom.xml└── src ├── main │ └── java │ └── cpprhtn │ └── App.java └── test └── java └── cpprhtn └── AppTest.java 6. 프로그램 생성프로젝트 디렉토리로 이동한후 다음 명령어를 실행시켜줍니다. 12cd study-hadoopmvn package 프로그램이 생성되면 target 디렉토리가 생성되며, 해당경로 안에 jar 파일이 만들어집니다. 7. 프로그램 실행target 디렉토리에 이동한 후에 아래의 코드를 실행합니다.jar 파일명은 각자에게 생성된 파일명을 적어주며, 실행할 클래스는 각자의 3-6의 패키지명.App입니다. 12cd targetjava -classpath study-hadoop-1.0-SNAPSHOT.jar cpprhtn.App 다음과 같이 출력되면 성공입니다. Hello World!","link":"/2022/01/21/start-maven-project/"},{"title":"맵리듀스란?","text":"맵리듀스는 HDFS에 저장된 파일을 분산 배치 분석을 할 수 있게 도와주는 프레임워크입니다. 개발자는 맵리듀스 프로그래밍 모델에 맞게 애플리케이션을 구현하고, 데이터 전송, 분산 처리, 내고장성 등의 복잡한 처리는 맵리듀스 프레임워크가 자동으로 처리해줍니다. 맵리듀스의 개념맵리듀스(MapReduce) 모델은 맵(Map)과 리듀스(Reduce)라는 두 가지 단계로 데이터를 처리합니다. Map : 입력 파일을 한 줄씩 읽어서 데이터를 변형(transformation) 데이터 변형 규칙은 자유롭게 정의가 가능 출력 또한 한 줄에 하나의 데이터가 출력 Reduce : Map의 결과 데이터를 집계(aggregation) 맵리듀스 프로그래밍맵리듀스 프로그래밍은 다음과 같은 함수로 표현됩니다. Map : (key1, value1) -&gt; list(key2, value2) 맵은 키(1)와 값(1)으로 구성된 데이터를 입력받아 이를 가공하고 분류한 후, 새로운 키(2)와 값(2)으로 된 리스트를 출력합니다. 이를 여러번 수행하면 새로운 키를 가진 여러개의 데이터가 생깁니다. Reduce : (key2, list(value2)) -&gt; (key3, list(value3)) 리듀스는 새로운 키(2)로 그룹핑된 값(2)의 리스트를 입력데이터로 전달받습니다. 그리고 값의 리스트에 대한 집계 연산을 실행해 새로운 키(3)로 그룹핑된 새로운 값(3)의 리스트를 생성합니다. MapReduce Architecture맵리듀스 프레임워크는 개발자가 분석 로직을 구현하는 데 집중하도록 돕고, 데이터에 대한 분산과 병렬 처리를 전담합니다. System configuration 클라이언트 클라이언트는 사용자가 실행한 맵리듀스 프로그램과 하둡에서 제공하는 맵리듀스 API를 의미합니다. 사용자는 맵리듀스 API로 맵리듀스 프로그램을 개발하고, 개발한 프로그램을 하둡에서 실행할 수 있습니다. 클라이언트가 하둡으로 실행을 요청하는 맵리듀스 프로그램은 잡이라는 하나의 작업 단위로 관리됩니다. 잡트래커 잡트래커는 하둡 클러스터에 등록된 전체 잡의 스케줄링을 관리하고 모니터링합니다. 전체 하둡 클러스터에서 하나의 잡트래커가 실행되며 보통 하둡의 네임노드 서버에서 실행됩니다. 사용자가 새로운 잡을 요청하면 잡트래커는 잡을 처리하기 위해 필요한 맵과 리듀스의 실행량을 계산합니다. 이렇게 계산된 맵과 리듀스를 어떤 태스크트래커에서 실행할지 결정하고, 해당 태스크트래커에 잡을 할당합니다. 이때 태스크트래커는 잡트래커의 작업 수행 요청을 받아 맵리듀스 프로그램을 실행합니다. 잡트래커와 태스크트래커는 하트비트라는 메서드로 네트워크 통신을 하면서 태스크트래커의 상태와 작업 실행 정보를 주고받게 됩니다. 만약 태스크트래커에 장애가 발생하면 잡트래커는 다른 대기 중인 태스크트래커를 찾아 태스크를 재실행합니다. 태스크트래커 태스크트래커는 사용자가 설정한 맵리듀스 프로그램을 실행하며, 하둡의 데이터노드에서 실행되는 데몬입니다. 태스크트래커는 잡트래커의 작업을 요청받고, 잡트래커가 요청한 맵과 리듀스 개수만큼 맵 태스크와 리듀스 태스크를 생성합니다. 여기서 맵 태스크와 리듀스 태스크는 사용자가 설정한 맵과 리듀스 프로그램을 의미합니다. 맵 태스크와 리듀스 태스크가 완성되면 새로운 JVM을 구동해 맵 태스크와 리듀스 태스크를 실행합니다. 이때 태스크를 실행하기 위한 JVM은 재사용할 수 있게 설정할 수 있습니다. 서버가 부족해서 하나의 데이터노드를 구성했더라도 여러 개의 JVM을 실행해 데이터를 동시에 분석하므로 병렬 처리 작업에 문제가 없습니다. Data flow1. 맵 단계맵 단계에서는 입력 파일을 읽어 맵의 출력 데이터를 생성하는 단계입니다. 아래와 같은 아키택처를 가지고 있으며 다음 순서에 따라 처리됩니다. 스플릿 : 맵리듀스는 HDFS에 저장된 파일을 읽어서 배치 처리를 합니다. 맵리듀스 프레임워크는 이러한 대용량 파일을 처리하기 위해 입력 데이터 파일을 입력 스플릿이라는 고정된 크기의 조각들로 분리합니다. 입력 스플릿별로 하나의 맵 태스크가 생성되며 각 입력 스플릿은 맵 태스크의 입력 데이터로 전달됩니다. 여기서 입력 스플릿은 HDFS 블록 크기 기준으로 생성됩니다. 따라서 현재 시점에서는 128MB 단위로 입력 스플릿이 생성될 것 입니다. 레코드 읽기 : 맵 태스크는 입력 스플릿의 데이터를 레코드 단위로 읽어서 사용자가 정의한 맵 함수를 실행합니다. 이때 맵 태스크의 출력 데이터는 태스크트래커가 실행되는 서버의 로컬 디스크에 저장되며, 맵의 출력키를 기준으로 정렬됩니다. HDFS에 저장하지 않는 이유는 맵 태스크의 출력 데이터는 중간 데이터라서 영구적으로 보관할 필요가 없기 때문이며, 잡이 완료될 경우 중간 데이터는 모두 삭제됩니다. 2. 셔플 단계리듀스 태스크는 맵 태스크의 출력 데이터를 내려받아 연산을 수행해야 합니다. 맵리듀스 프레임워크는 이러한 작업이 진행될 수 있게 셔플을 지원합니다. 셔플은 맵 태스크의 출력 데이터가 리듀스 태스크에게 전달되는 일련의 과정을 의미하며, 다음 순서에 따라 진행됩니다. 파티셔너는 맵의 출력 레코드를 읽어서 출력키의 해시값을 구합니다. 각 해시값은 레코드가 속하는 파티션 번호로 사용됩니다. 파티셔는 실행될 리듀스 태스크 개수만큼 생성됩니다. 파티셔닝 된 맵의 출력 데이터는 네트워크를 통해 리듀스 태스크에 전달됩니다. 하지만 모든 맵의 출력이 동시에 완료되지 않기 때문에 리듀스 태스크는 자신이 처리할 데이터가 모일 때까지 대기합니다. 리듀스 태스크는 맵의 출력 데이터가 모두 모이면 데이터를 정렬하고, 하나의 입력 데이터로 병합합니다. 리듀스 태스크는 병합된 데이터를 레코드 단위로 읽어 들입니다. 3. 리듀스 단계리듀스 단계에서는 사용자에게 전달할 출력 파일을 생성합니다. 다음과 같은 두 단계를 따릅니다. 리듀스 태스크는 사용자가 정의한 리듀스 함수를 레코드 단위로 실행합니다. 이때 리듀스 태스크가 읽어 들이는 데이터는 입력키와 입력키가 해당하는 입력값의 목록으로 구성됩니다. 위 예제의 리듀스 함수는 입력키별로 입력값의 목록을 합산해서 출력합니다. 이때 출력 데이터는 출력키와 출력값의 쌍으로 구성됩니다. 리듀스 함수가 출력한 데이터는 HDFS에 저장됩니다. HDFS에는 리듀스 개수만큼 출력 파일이 생성되며, 파일명은 part-nnnnn으로 설정됩니다. 여기서 nnnnn은 파티션 번호를 의미하며, 00000부터 1씩 증가합니다.","link":"/2022/01/23/What-is-MapReduce/"},{"title":"Mapreduce-Job-Process","text":"맵리듀스 잡은 클라이언트가 잡 실행을 요청하는 단계, 해당 잡이 초기화되는 단계, 잡을 실행하기 위한 태스크를 할당하는 단계, 할당된 태스크가 실행되는 단계, 마지막으로 잡이 완료되는 단계로 구성됩니다. 1. 잡 실행 요청맵리듀스 애플리케이션인 클라이언트가 잡 실행을 요청하면 아래와 같이 작업이 진행됩니다. 클라이언트는 org.apache.hadoop.mapreduce.Job의 waitForCompletion 메서드를 호출해 잡 실행을 요청합니다. 이때 클라이언트의 요청은 Job의 내부 컴포넌트인 JobClient에 전달됩니다. JobClient는 잡트래커의 getNewJobId 메서트를 호출해 새로운 잡ID를 요청합니다. 잡트래커는 잡의 출력 파일 경로가 정상적인지 확인한 후 잡ID를 발급합니다. 참고로 JobClient와 잡트래커는 RPC로 통신하며, JobSubmissionProtocol에 정의된 프로토콜을 사용합니다. 클라이언트는 잡을 실행하는 데 필요한 정보를 잡트래커와 태스크트래커에게 공유해야 합니다. 그래서 JobClient는 다음과 같은 파일을 HDFS에 저장합니다. 입력 스플릿 정보 JobConf에 설명된 정보 잡 클래스 파일 혹은 잡 클래스가 포함된 JAR 파일 JobClient는 잡트래커의 submitJob 메서드를 호출해 잡 실행을 요청합니다. 2. 잡 초기화잡트래커는 잡을 실행하기 위한 초기 설정 작업을 진행합니다. 각 과정은 아래와 같이 진행됩니다. 잡트래커는 잡의 상태와 진행 과정을 모니터링할 수 있는 JobInProgress를 생성합니다. JobInProgress는 JobClient가 HDFS에 등록한 잡 공통 파일을 로컬 디스크로 복사한 후, 스플릿 정보를 이용해 맵 태스크 개수와 리듀스 태스크 개수를 계산합니다. 또한 잡의 실행 상태를 RUNNING으로 설정합니다. 잡트래커는 1단계에서 생성한 JobInProgress 객체를 내부 큐인 jobs에 등록합니다. 큐에 등록된 JobInProgress는 스케줄러에 의해 소비됩니다. 3. 태스크 할당맵리듀스는 태스크를 할당하기 위한 스케줄러인 TaskScheduler를 제공합니다. TaskScheduler는 추상 클래스이며, 이를 구현한 FIFO 방식의 JobQueueTaskScheduler, 풀 방식의 FairScheduler, 다중 큐 방식의 CapacityScheduler를 제공합니다. 기본 스케줄러는 FIFO 방식의 스케줄러를 이용해 잡의 실행 순서대로 태스크가 할당됩니다. 과정은 아래와 같이 진행됩니다. 태스크트래커는 3초에 한 번씩 잡트래커에게 하트비트 메시지를 전송하며, 이를 통해 태스크트래커가 실행 중이라는 것과 새로운 태스크를 실행할 준비가 됐다는 것을 알려줍니다. 스케줄러는 태스크트래커의 하트비트 메시지를 확인한 후, 내부 큐에서 태스크를 할당할 잡을 선택합니다. 그리고 해당 잡에서 하나의 태스크를 선택합니다. 이때 잡의 선택은 각 스케줄러의 알고리즘에 맞게 선택하게 됩니다. 스케줄러는 맵 태스크와 리듀스 태스크를 구분해 태스크를 할당합니다. 우선 맵 태스크의 경우 입력 스플릿과 동일한 서버의 태스크를 선택합니다. 이는 네트워크를 통하지 않고, 로컬 디스크에 접근해서 높은 성능을 낼 수 있기 때문입니다. 차선으로는 동일한 랙의 태스크를 선택합니다. 리듀스 태스크의 경우는 대부분 맵 태스크의 출력 데이터를 네트워크로 내러받기 때문에 단순히 태스크 목록에 있는 순서대로 선택하게 됩니다. 스케줄러는 태스크를 선택한 후 해당 태스크트래커에게 태스크 할당을 알려줍니다. 잡트래커는 태스크트래커가 전송한 하트비트의 응답으로 HeartbeatResponse를 전송합니다. 잡트래커는 태스크트래커에게 지시할 내용을 HeartbeatResponse에 설정할 수 있습니다. 그래서 스케줄러는 HeartbeatResponse에 태스크 실행을 요청합니다. 참고로 HeartbeatResponse에는 태스크 실행, 태스크 종료, 잡 종료, 태스크트래커 초기화 재실행, 태스크 완료 등의 작업을 설정할 수 있습니다. 4. 태스크 실행태스크트래커는 할당받은 태스크를 새로운 JVM에서 실행하게 되며, 맵리듀스는 이를 Child JVM이라고 표현합니다. 이때 새로운 JVM에서 발생하는 버그는 태스크트래커에게 영향을 끼치지 않아서 안정적으로 태스크트래커를 운영할 수 있습니다. 또한 사용자가 원할 경우 매번 JVM을 새로 생성하는 것이 아니라 재사용하게 할 수도 있습니다. TaskLauncher는 HeartbeatResponse에서 태스크 정보를 꺼내서 태스크의 상태와 진행 과정을 모니터링할 수 있는 TaskInProgress를 생성합니다. 태스크트래커는 HDFS에 저장된 잡 공통 파일들을 로컬 디렉터리로 복사합니다. 그리고 TaskInProgress는 태스크 실행 결과를 저장할 로컬 디렉터리를 생성한 후 잡 JAR 파일을 이 디렉터리에 풀어 놓습니다. TaskInProgress는 TaskRunner에게 태스크 실행을 요청합니다. TaskRunner는 JvmManager에게 차일드 JVM에서 태스크를 실행해줄 것을 요청합니다. JvmManager는 실행할 클래스명과 옵션을 설정한 후, 커맨드 라인에서 차일드 JVM을 실행합니다. 이때 차일드 JVM은 TaskUmbilicalProtocol 인터페이스로 부모 클래스와 통신하게 됩니다. 차일드 JVM은 태스크가 완료될 때까지 태스크의 진행 과정을 주기적으로 JvmManager에게 알려줍니다. 태스크트래커는 이 정보를 공유받아 태스크의 진행 과정을 모니터링 가능합니다. 사용자가 정의한 매퍼 클래스 혹은 리듀서 클래스가 실행됩니다. 5. 잡 완료 태스크트래커가 잡트래커게 전송하는 하트비트에는 완료된 태스크의 정보가 포함됩니다. 잡트래커는 해당 잡이 실행한 전체 태스크의 완료 정보를 받게 될 경우 JobInProgress는 잡의 상태를 SUCCEEDED로 변경합니다. 만약 장애 때문에 잡이 실패했다면 잡의 상태를 FAILED로 변경합니다. 잡을 실행한 클라이언트와 JobClient는 잡이 완료될 때까지 대기하고 있으며, JobClient는 잡트래커의 getJobStatus 메서드를 호출해 잡의 상태를 연속해서 확인합니다. JobClient는 잡의 상태가 SUCCEED면 true, FAILED면 false를 클라이언트에게 전달합니다. 클라이언트는 최종 결과를 출력하고, 잡 실행을 완료합니다.","link":"/2022/02/09/Mapreduce-Job-Process/"},{"title":"MML Translation","text":"최근에 Mathematics For Machine Learning (MML)이라는 책을 번역해보는 팀에 참여하게 되었습니다. 책 주소: https://mml-book.github.io/ MML이라는 책은 머신러닝을 수학적으로 자세히 다루는 책입니다. 나와있는 책은 전부 영어로 되어있었고, 이를 한글로 번역한다면 읽게되는 독자층이 한층 더 넓어지지 않을까? 생각하여 진행하게 되었습니다. 찾아보니 개인이 조금씩 번역하여 포스팅하는 블로그들이 존재하였으나, 속도도 매우 느렸고 책의 반의 반도 포스팅되지 않았기 때문에 여러명이 모여서 12장까지를 한꺼번에 번역하여 포스팅 해보고싶었습니다. 그렇게 하여 10명이 모여서, 세부목표를 먼저 세웠습니다. 번역을 어떻게 할 것인지에 대한 기준도 세우고, 언제까지 얼마만큼을 번역할것인가, 그리고 검수는 누가 할 것이며, 포스팅은 어디에다가 할 것인가 등등에 대해 회의를 통해서 정했습니다. 각자 맡은 파트를 조금씩 번역하던 와중에 우리가 번역하는 것을 이 책을 쓴 작가분도 알아야 하지 않을까?라는 의문이 들어 “우리가 이러한 이유로 이 책을 번역해보고싶다”라는 내용의 메일을 보냈습니다. 몇일뒤 답변이 왔는데, 이미 다른 출판사에서 번역권에 대한 라이선스를 가지고 있다는 내용의 답변을 받았습니다. 저작권 문제와 다양한 문제로 바로 중단되었습니다. 결국 중단되긴 했지만, 이 책을 번역하기 위해서 더 깊이 공부했던 것들, 그리고 주변 사람들에게 물어보았던 다양한 질문들이 나에게 큰 경험이 되어서 좋았습니다.","link":"/2021/04/30/MML-translation/"},{"title":"Tensorflow Certificate 자격증 준비 과정 및 후기","text":"이번에 기회가 되어서 Tensorflow Certificate 자격증을 따게 되었습니다. 이 자격증의 준비 과정과 후기를 간단하게나마 기록으로 남기려고 글을 써봅니다. 시험 준비먼저 이 자격증은 Tensorflow 2.X를 다룰 수 있다는 자격을 나타내는 증명서 같은 느낌이고, 구글에서 인증해주는 시험입니다. 시험 응시료는 100달러이며, 한화 약 115,000원 정도가 들었습니다. 학생의 신분으로써는 “한번만에 무조건 성공해야지”라는 마음이 들게 한 가격이였습니다. 시험 환경은 파이참으로 맞춰야 합니다. Tensorflow를 설치하고, Test project를 만들어서 잘 설치되었는지 심플한 코드를 짜서 한번 돌려보시길 바랍니다. (참고로 저는 설치만 하고 바로 시험쳤더니.. 중간에 에러가 뜨더라구요.. 이런 경우의 해결법은 아래 적어두었습니다.) 플러그인에서 Tensorflow라고 검색하시면 Tensorflow Exam 플러그인이 뜨는데, 그것을 설치받으면 시험칠 준비는 다 완료되었습니다. 물론 Tensorflow2 공부도 해야합니다. 하지만 저는 3년간 Tensorflow를 다뤄온 경험으로.. 공부는 생략했습니다.. 좀더 자세한 준비방법은 여기를 참고하세요. 시험 내용시험 내용은 구체적인 언급은 하지 않겠지만 큰 틀만 설명하겠습니다. 총 주어지는 응시 시간은 5시간입니다. 하지만 그 이전에 제출 가능합니다. 1차원 선형회귀 1문제, CNN 2문제, 자연어처리 1문제, LSTM 1문제로 구성되어 있습니다. skeleton code가 이미 짜여있고, 응시자가 구현해야 할 부분에는 주석으로 처리되어 있습니다. 주어진 문제를 다 읽고나면 요구대로 코드를 작성하면 됩니다. 코드를 작성하고 나면 파이참에서 그 코드를 실행하여 .h5 확장자의 파일하나를 생성해야 합니다. (.h5파일은 코드가 잘 돌아갔다면 저절로 생성됩니다.) 해당문제를 채점(제출아닙니다.)해볼 수 있는데, 채점해보면 0~5점 사이의 점수를 받을 수 있습니다. 여러번 채점해봤을때, 모두 5점이 나온다면 잘 풀은것이고, 간혹(이라 하고 자주…라고 합시다) 점수가 왔다갔다 할 수 있는데, 부족한 모델이라고 해석하고, 모델쪽을 더 손보시길 바랍니다. 채점을 클릭하면, .h5파일이 전송되어 채점된다고 느꼈습니다. (이부분은 제 추측입니다.) 여기서 문제점은 .h5파일을 만들기 위해서 코드를 run했을때, local에서 돌아가기 때문에, 성능이 안좋을수록 compile시간이 길어지고, 그러면 채점해볼 횟수도 줄어드니 불공평하지 않을까라는 생각이 들었습니다. 어쩔수 없는 부분이지만, 되도록이면 GPU가 좋은곳에서 시험을 치는게 좋을것 같습니다. 또 알려드리고 싶은 한가지로는 시험 중간에 뜨는 에러입니다. 코드의 에러가 아니라, 개발 환경적인 에러가 발생한 경우에 대해 제 경험을 말해드리겠습니다. 우선 제 경우는 특정 라이브러리를 install 하라는 상태였고, 그래서 해당 라이브러리를 설치했으나, 여전히 인식을 못하는 상황이였습니다. 30분정도 이것저것 시도하다가 생각해낸 방법은 구글 코랩을 쓰는 방법이였습니다. 자신의 구글 드라이브와 마운트 해놓은 상태로 해당 코드를 돌리면 시간은 좀 느릴 수 있으나, 코드를 돌려 .h5파일을 얻을 수 있고, 저는 이 .h5파일을 다운받아 시험치는 프로젝트의 해당 번호의 폴더에 넣어주었습니다. 다행히도 잘 채점되었습니다. 혹시라도 저와 같은 상황이 발생한다면 이런 방법도 시도해보시길 바랍니다. 어찌저찌해서 저는 코드자체는 순조롭게 작성을 하였고, 2시간만에 5문제 모두 만점을 받아서 제출하고 시험을 종료했습니다. 시험 종료후시험 제출이 완료되었다면, 본인의 메일에 설문지가 하나 오고, 거기에 본인의 정보를 입력하여 제출하면, 2주이내로 자격증을 발급해줍니다. (저는 3일만에 발급이 되더라구요. 시험 합격이 아닐 경우는 잘 모르겠네요..) 또한 여기에서 본인의 정보가 기록됩니다. 참고로 위 사이트에 본인 정보가 기록되는것은 자격증 발급시기보다 한참 뒤에 기록이 되더군요. 최소 2주뒤에 본인을 검색해보시길 바랍니다. 시험 자체에 대한 후기를 남기자면, 시험전에 Simple한 Tensorflow 코드가 잘 돌아가는지 확인하고 응시하는게 좋을듯하고, CNN, 자연어처리, LSTM 코드의 명확한 이해는 필요해 보입니다. 점수 배점은 자세히는 모르겠지만, 만점을 받으려면 엄청 쉬운 난이도는 아니였습니다. 100달러의 가치를 하는지는 잘 모르겠지만, 처음에 얘기한대로, TF2를 다룰줄 안다는 자격의 의미로 이해하면 될 듯하고, CNN, 자연어처리, 시계열에 대한 어느정도의 지식이 있구나라고 나타낼 수 있는 자격증이다라고 생각합니다 그리고 포트폴리오 자격증부분에 한줄 더 쓰이기도 하구요 ^^ (사실 이 목적이 가장 크지 않았을까 싶네요) 글은 이정도에서 마무리하겠습니다. 다들 무사히 자격증 따시길 바랍니다~.","link":"/2021/06/13/tensorflow-certificate/"},{"title":"Jetson Board and Pixhawk 4 Setting","text":"Jetson SettingInstall MAVROSYour OS version &gt;= Ubuntu 18.04| Install MAVROS Melodic 1sudo apt-get install ros-melodic-mavros ros-melodic-mavros-extras 1234mkdir -p ~/catkin_ws/srccd ~/catkin_wscatkin initwstool init src 1sudo apt-get install python-catkin-tools python-rosinstall-generator -y 1wstool init ~/catkin_ws/src | Install MAVLink Melodic 1rosinstall_generator --rosdistro melodic mavlink | tee /tmp/mavros.rosinstall 1rosinstall_generator --upstream mavros | tee -a /tmp/mavros.rosinstall 123wstool merge -t src /tmp/mavros.rosinstallwstool update -t src -j4rosdep install --from-paths src --ignore-src -y 1./src/mavros/mavros/scripts/install_geographiclib_datasets.sh 1catkin build 1source devel/setup.bash Your OS version &lt; Ubuntu 18.04| Install MAVROS Kinetic 1sudo apt-get install ros-kinetic-mavros ros-kinetic-mavros-extras 1234mkdir -p ~/catkin_ws/srccd ~/catkin_wscatkin initwstool init src 1sudo apt-get install python-catkin-tools python-rosinstall-generator -y 1wstool init ~/catkin_ws/src | Install MAVLink Melodic 1rosinstall_generator --rosdistro kinetic mavlink | tee /tmp/mavros.rosinstall 1rosinstall_generator --upstream mavros | tee -a /tmp/mavros.rosinstall 123wstool merge -t src /tmp/mavros.rosinstallwstool update -t src -j4rosdep install --from-paths src --ignore-src -y 1./src/mavros/mavros/scripts/install_geographiclib_datasets.sh 1catkin build 1source devel/setup.bash","link":"/2021/06/13/jetson-px4/"},{"title":"About Game Programming","text":"Ross Bae를 통해 들은 이야기게임업계는 콘텐츠 기반의 수익구조를 낸다. 따라서 게임개발자 역시 기획된 콘텐츠를 따라 갈 수 밖에없다. 또한 게임 엔진은 한정 되어 있기 때문에 엔진단에서의 기능개선 등에는 많은 어려움이 존재한다. 콘텐츠개발 이외의 영역에서는 게임의 유지보수, 최적화, 안티치트에 대한 개발을 주로 진행하게 된다. 유지보수는 어떠한 배포 단에서든 모두 필요한 요소이므로 설명을 생략하겠다. 최적화는 개발단에서 상당히 중요한 요소가 될 것이다. 최적화는 특히 해당 게임을 플레이하는 유저들에게 체감되는 부분이다. 더 낮은 사양에서 원활하게 돌아가게 하거나, 충돌 등의 부분에서 문제가 없는지 확인한다. PC와는 다르게 제한된 성능의 콘솔 게임은 특히 최적화가 더 중요해진다. 제한된 성능위에서 배포하는 게임이 원활하게 돌아가기 위해서는 해당 콘솔 사용 유저단에서 할 수 있는 방법은 거의 없고, 해당 게임을 배포하는 게임회사측에서 최적화를 기다릴 수 밖에 없다. 예를 들어서 사이버펑크 2077 이라는 게임역시 고사양의 PC 스팩을 요구할 뿐만 아니라 콘솔로도 출시는 했지만, 정작 콘솔의 제한된 성능덕분에 콘솔 유저는 게임을 플레이할 수 었는 상황이 발생했다. 또 다른 예시로는 배틀그라운드가 있다. 배그역시 초기에는 콘솔에서 상당히 힘들어하는 부분이 존재하였으나, 현재시점에서는 콘솔에서도 원활하게 돌아가도록 최적화를 당겨온 상태이다. 마지막으로 유저 입장에서 또 다른 문제점인 핵, 혹은 치트라 부르는 부분에 대해서 설명한다. 핵, 치트는 해당 게임의 본질을 흐릴 뿐 아니라, 사용하는 소수의 유저들을 통해서 일반 유저들을 떠나게 만드는 주요 원인이 된다.이를 방지하기 위해서 치팅을 탐지하고 밴을하는 안티치팅 개발에도 많은 인력이 소모된다. 안티치팅은 PC위에서만 돌아가는 구조일 수 밖에 없다. 왜냐하면 콘솔에서는 한 가지의 어플리케이션만 구동되는 형태이기 때문에, 현재 게임을 플레이하고 있다면, 추가적인 치팅 프로그램을 돌릴 수 없기 때문이다. 따라서 PC에대한 문제점을 해결해야하는데, 치팅의 최종적인 방법은, 게임 파일 일부를 바꿔치거나, PC의 더 높은 컨트롤 권한을 뺏는 방법이다. 간단한 치팅부터 교묘한 치팅까지 다양한 방법이 있지만, 찾기 어려운 치팅일 수록 어셈블리 단에서의 작업이 요구되고 이는 상당히 대응이 늦어지는 상황이 발생할 뿐만 아니라, 어디부분에서의 문제점이 일어났는지를 모르기 때문에 상당한 시간과 디버깅을 요구한다. 이런 이야기를 읽다보면 모순된 점을 발견할 수 있을 것이다. 게임 수익구조는 컨텐츠에 있으나, 컨텐츠는 소모성이 크므로 꾸준하게 새로운 컨텐츠를 제작하여 배포해야한다. 하지만 컨텐츠 생산성에만 신경을 쓰면 일명 개적화 상태가 되어서 한정된 리소스를 써야하는 플스와 같은 환경에서 큰 이슈가 종종 일어나기도 하며, PC에서는 다양한 핵 프로그램이 판을 치는 상황이 된다. 따라서 생각보다 많은 개발인력이 필요하며, 세분화된 영역속에서 작업을 하는것이다. 사실 해당 이야기를 듣고 한참지나서 글을 마무리하려하니까 아무 생각이 안나서 마무리가 이상해진 글..","link":"/2022/05/01/About-Game-Programming/"},{"title":"Single Shot MultiBox Detector(SSD)","text":"2022 Open Source Contribution Academy 의 Pytorch Hub 번역 팀의 일원으로 Single Shot MultiBox Detector(SSD) 논문에 대한 정리 및 예제 모델 학습을 해보았습니다. 논문 배경 상황 설명SSD는 Object Detection을 목표로 ECCV’ 16에 게제된 Paper 입니다. arxiv에는 2015/12월에 게제 되었으며, 근처에 나온 관련 논문으로는 Fast R-CNN (ICCV' 15), Faster R-CNN (NIPS' 15), YOLO v1 (CVPR' 16)정도가 있습니다. 따라서 본 논문의 Introduction에서도 Faster R-CNN, YOLO(v1) [당시에는 YOLO가 처음 나온 시기여서 version이 따로 붙어있지 않은 상태. 현재는 v7까지 나와있음.] 과 비교하는 모습을 볼 수 있습니다. 당시의 시점에서 본 논문을 해석해봅시다. 당시의 YOLO는 1-stage detector로 빠른 detection이 가능하였고(상대적으로 정확도가 낮음), R-CNN 기반의 2-stage detector 모델들은 selective search기반의 region proposals들을 추출하여 더 정확한 detection이 가능하다고 제안된 논문이였습니다(상대적으로 속도가 느림). 하지만 SSD는 기존에 제안된 논문들보다 더 빠르고 더 정확한 결과를 보여주었습니다. 1. Model SSD는 base network와 auxiliary structure network로 이루어진 1-stage detector 입니다.VGG-16 network를 base network로 사용하였고, auxiliary structure network로 Convolutional network를 사용하였습니다.SSD는 VGG-16 network에서 많은 파라미터를 요구하는 fc레이어를 사용하지 않고 convolution layer로 대체하여 base network와 auxiliary structure network를 연결하였습니다. Multi-scale feature maps for detection YOLO의 7×7 single scale feature map과는 다르게 multi-scale feature maps을 사용하였습니다.Base network의 마지막 부분에 convolution layer들을 추가하는데, 해당 layer들은 점진적으로 size가 감소하며, 이는 다양한 scale에 대해 뛰어난 detection 성능을 만들어냅니다. 논문에서는 6종류의 scale을 가지는 feature maps를 사용하였습니다. (38×38, 19×19, 10×10, 5×5, 3×3, 1×1) Convolutional predictors for detectionpchannels를 갖는 m×n의 feature map에 3×3×p크기의 convolution filter를 적용하였습니다.Detection(Output)은 default bounding boxes의 category score와 box offsets을 측정합니다.SSD는 Detection까지 convolution 연산을 하는 반면, YOLO는 fc 연산을 거치기 때문에 연산량과 속도 측면에서 효과적이였습니다. Default boxes and aspect ratios각각의 feature map cell마다 서로 다른 크기의 scale과 aspect ratio(종횡비)를 가지는 default box를 생성합니다. [Default boxes는 Faster R-CNN의 anchor boxes와 비슷합니다. 하지만 6종류의 scale을 가진 feature map의 각각의 cell마다 default boxes를 생성한다는 점이 다릅니다.]각각의 cell은 (c+4)k 개의 예측을 합니다. c는 class 수를, k는 default boxes의 개수를, 4는 4개의 offset를 의미합니다. 이를 m×nfeature map에 적용하면 (c+4)k m n 개의 output이 출력됩니다. 2. TrainingMatching strategyTraining을 진행하기위해 default boxes를 ground truth와 매칭합니다. 본 논문에서는 jaccard overlap(또는 Intersection Over UnionIOU)값이 0.5 이상인 boxes들을 positive(Object가 있는/negative는 Object가 없는. 즉 배경을 의미) boxes로 사용합니다. Training objectiveLoss function은 localization loss(loc) 와 confidence loss(conf)의 가중 합 입니다. N: number of matched default boxes xijp = {1, 0}: indicator for matching the i-th default box to the j-th ground truth box of category p / positive box = 1, negative box = 0 l: predicted box g: ground truth box d: default box cx, cy: center x, y w: width h: height a: 1 Choosing scales and aspect ratios for default boxes위 그림의 (b)와 (c)를 보면 feature map의 크기가 작아질수록 더 큰 object detection이 가능함을 알 수 있습니다. 본 논문에서는 이러한 default box의 scale을 다음과 같이 정의합니다. m: Number of scale feature map (6)s&lt;sub&gt;k&lt;/sub&gt;값은 원본 이미지에 대한 비율을 의미하며, 해당 수식을 통해 다양한 크기의 default box를 생성할 수 있습니다. ex) 300×300 원본 이미지에 대하여 s = 0.1이고 aspect ratio가 1:1이면 default box의 size는 30×30이 됩니다. Hard negative mining이미지를 생각해보면 object보다 background의 비중이 훨씬 높을겁니다. 이러한 상황을 반영하여 positive와 negative의 비율을 1:3으로 하여 데이터를 사용합니다. Data augmentationRobust한 모델을 얻기 위하여 data augmentation을 수행하였습니다. Result from the PaperPASCAL VOC, COCO datasets에서 속도(FPS)와 정확도(mAP)가 가장 높은 모델이였습니다. (16년도 기준)아래표에서 Fast YOLO는 2017에 제안된 논문으로 정확도를 포기하고 속도에만 중점을 둔 모델입니다. 최근에 제안된 YOLO(single-scale)나 Swin(mulit-scale)에게 처참히 밀려있는 근황입니다. SSD 모델 사용해보기Pytorch Hub - SSD를 이용하여 쉽고 빠르게 모델을 불러와서 사용해볼 수 있었습니다. 123import torchssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils') /usr/local/lib/python3.7/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. /usr/local/lib/python3.7/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour \"You are about to download and run code from an untrusted repository. In a future release, this won't \" Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available \"pytorch_quantization module not found, quantization will not be available\" /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available \"pytorch_quantization module not found, quantization will not be available\" /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead. f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \" /usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth 100% 97.8M/97.8M [00:00","link":"/2022/07/13/Single-Shot-MultiBox-Detector-SSD/"},{"title":"About Block Chain","text":"블록체인에 관심을 가지게 된 계기는.. 주변에 블록체인 회사로 이직하면서 억대연봉을 받기 시작한 한 형 덕분이다. 블록체인에 대해 알아본 내용과, 개인적인 생각을 정리해서 글로 포스트해본다. 비트코인기존의 중앙화(Centralization)된 시스템 기반의 화폐, 전자화폐, 금융상품 등과 달리 가상화 및 분산처리가 가능한 비트코인은 블록체인 기술을 바탕으로 신뢰성과 안전성이 확보된 탈-중앙화(De-centralization) 시스템을 구축한다. 블록체인은 권한(Permission) 및 합의 방식(Consensus Algorism)에 따라서 퍼블릭 블록체인(Public Blockchain), 프라이빗 블록체인(Private Blockchain), 컨소시엄 블록체인(Consortium Blockchain)으로 분류된다. 퍼블릭 블록체인은 비트코인(Bitcoin)이나 이더리움(Ethereum)과 같이 누구나 네트워크에 참여하여 거래내역은 물론 네트워크에서 발생하는 여러 행위들이 공유되는 공개형 블록체인이고, 이와 반대로 프라이빗 블록체인은 폐쇄형 블록체인으로 허가된 참여자 외에 거래내역과 행위들이 공유되지 않는다. 컨소시엄 블록체인은 퍼블릭 블록체인과 프라이빗 블록체인의 성격이 결합된 형태로 반-중앙형 성격을 띄게 된다. 블록체인은 Blockchain Scalability Trilemma를 기반으로 3가지의 방향에서 평가할 수 있다. 탈중앙화, 보안성, 확장성의 3가지의 방향에서 블록체인을 평가할 수 있으며, 이 세가지를 모두 만족시키는 것은 불가능하다. 탈중앙화란?탈중화의 분산된 절차와 기술은 여러 부문에 걸쳐 중개인의 역할을 줄이고 사용자에게 이익을 준다. 한 예로, 분산 금융(DeFi) 시스템은 금융 상품에서 은행 기관을 제거함으로써 사용자에게 더 큰 수익이 돌아갈 수 있다. 반면에 최적의 탈중앙화를 얻으면 네트워크 처리량이 감소하는 경향이 있다. 더 많은 채굴자가 합의를 통해 네트워크를 보호할수록 거래 속도가 느려진다. 즉 탈중앙화가 커질수록 사용자에게 큰 이익이 돌아가는 대신에 거래 속도와 거래량이 감소한다. 보안성이란?블록체인 네트워크에서 네트워크 성능을 향상시키기 위해 블록체인 노드를 최소화한다. 그러나 이는 네트워크의 보안을 약화시키게 된다. 노드 배포가 제한된 개방형 네트워크에서 합의에 도달하면 해커가 해싱 파워를 쉽게 수집할 수 있기 때문에 공격이 발생할 가능성이 더 높다. 해커는 네트워크를 가로채고 과부하를 통해 재정적 이익을 위해 거래에 영향을 줄 수 있다. 실제 사례로, 2020년 8월, 이더리움과 무관한 이더리움 클래식(ETC) 블록체인이 4,000개 이상의 블록을 재구성하는 공격을 받아 수백만 달러의 손실을 입었다. 확장성이란?블록체인 프로토콜의 확장성은 높은 트랜잭션 처리량과 향후 확장을 유지할 수 있는 능력을 나타낸다. 이는 사용 사례가 증가하고 블록체인 기술 채택이 증가함에 따라 확장 가능한 블록체인의 성능이 저하되지 않음을 의미한다. 채택이 증가함에 따라 제대로 작동하지 않는 블록체인은 확장할 수 없는 것으로 간주된다. 확장성은 블록체인 네트워크가 현재 훨씬 더 빠른 네트워크 결제 속도와 사용성을 제공하는 오래된 중앙 집중식 시스템과 경쟁할 수 있는 유일한 옵션이다. 하지만 높은 확장성은 탈중앙화 또는 보안성 둘 다를 감소시켜야 한다. 블록체인 기술은 많은 보안적인 문제가 존재한다. SCSU에서 발표한 논문 “Security Threats Classification in Blockchains” 53페이지 Table을 보면 5개의 큰 Security Threats와 30개의 세부적인 Attack Vectors로 나누었다. 실제 보안 취약점으로 인한 큰 피해 사례 예시로는 Wallet Threats, Smart Contracts Threats 정도가 있다. 첫 번째로 뽑은 큰 보안이슈는 Wallet Threats다. 암호화폐 지갑은 일상에서 사용하는 지갑의 기능과 같이 돈을 저장하는 것이 아니라 개인키(Private Key), 공개키(Public Key) 그리고 자산을 관리하는 소프트웨어 프로그램을 의미한다. 공개키가 보인 소유의 계좌번호이고, 개인키가 계좌 비밀번호와 유사한 역할을 수행하기 때문에 개인키를 소유하는 것은 공개키에 따른 자산의 모든 관리권한을 의미하게 된다. 이러한 개인키가 탈취가되면 탈취한 악의적 사용자는 개인키에 저장된 암호화폐를 본인인 것처럼 사용할 수 있게된다. 실제로도 2014년 12월에 발생한 암호화폐거래소의 경우에도 거래소 서버가 공격 당해서 개인키 파일이 유출되었고, 이로 인해 월렛에 존재하던 18,866 비트코인이 손실되었다. 두 번째로는 Smart Contracts Threats다. 스마트 컨트랙트(Smart Contracts)는 서면으로 작성되어 있는 기존 계약서(Contract)를 중개자 없이 P2P로 ‘스스로 이행이 되는 자동화된 약정(Automated Self-Enforced Agreements)’ 행위를 의미한다. 금융, 지적재산권, 물류 유통과 같이 상호 거래 및 검증이 빈번하게 발생하고 업무 자동화가 가능한 시스템이 대표적인 업무영역이라고 볼 수 있다. 스마트 컨트랙트는 기존의 계약과 관련된 거래 비용의 감소와 합의에 따른 신뢰를 바탕으로 보안성의 장점을 앞세워 다양한 서비스들이 출시되었지만 2016년에 발생한 DAO(Decentralized Autonomous Organization) 해킹으로 기술적 한계가 드러났다. 특정 계정의 잔액과 합계를 갱신하는 splitDAO()함수가 재호출되기 이전에 어떠한 함수라도 호출시키면 무한으로 자금을 이동시킬 수 있는 재귀함수의 취약점이다. A계좌에 있는 100만원을 B계좌로 50만원 출금신청을 하는 경우 A계좌의 100만원에서 50만원을 차감하기 전에 다시 B계좌로 출금을 시도하고 다시 또 50만원을 출금하는 행동을 반복하면 A계좌에서는 50만원을 차감하기 전에 새로운 출금요청이 왔기 때문에 100만이 그대로 존재하게 되고 B계좌에는 출금한 50만원들이 꾸준히 쌓이게 되는 방법이다. 이러한 해킹이 이더리움 코인을 대상으로 이루어졌고 이는 1억 5천만 달러이상의 손해를 불러 일으켰다. 이더리움측은 해커들이 탈취한 코인을 현금으로 바꾸지못하도록 소프트포크를 하였으며, 얼마뒤에는 도난당한 코인을 평행세계 개념인 하드포크 방식을 통해 분리해냈는데, 하드포크를 동의하지 않은 사람들의 코인이 이더리움 클래식이라는 이름으로 분리되었다. 이렇게 해킹당한 코인은 이더리움 클래식이라는 이름으로 남게되었고, 얼마뒤 해당 이더리움 클래식 보유자들은 DAO 토큰으로 교환을 할 수 있게 되었다. 당시 이더리움 클래식의 환산금액은 약 850만 달러로, 실질적으로 해커가 탈취해간 금액일 것이다. 이러한 기술적인 보안공격 뿐만 아니라 최근에는 경제, 금융학적인 공격으로 인한 피해사례도 나타나게 되었다. 2022년 5월 12일, 테라-루나 사태는 일주일 사이에 58조원이 증발한 사례이다. 국내에서 만든 코인으로 1테라를 1달러에 상응하도록 루나코인의 양을 조절하여 큰 신뢰를 얻었던 코인이였고, 한때 글로벌 가상화폐 시가총액 10위안에 들던 코인이였으나 거대한 공매도 세력이 대량으로 공매도 물량을 풀면서 인위적으로 시세 하락을 조장하였고, 그 여파로 디페깅이 유도되었다는 의견이다. 알고리즘 페깅은 프로그래밍화된 코인 운영 시스템이 시세를 감지하여 거래량을 조절하는데, 그 조절 능력을 뛰어넘는 정도의 투매를 하면 시세가 갑자기 폭락하게 되고, 그 회복에도 시간이 걸릴 수밖에 없게 된다. 이러한 디페깅 상태가 지속되면 테라 자체에 대한 신뢰가 떨어지고, 무담보로 운영되는 알고리즘 페깅의 특성상 돈을 떼일 수 있다는 투자자들의 공포 심리를 유발시키게 되며, 이 영향으로 수만은 투자자들이 발을 빼며, 연쇄적인 코인하락이 발생하며 일주일사이 99%이상의 하락으로 발생하게된다. 또한 실제로 블록체인을 이용하는 DApp이라는 앱의 한 줄기도 생겼는데, 기존의 소프트웨어들은 보안적인 관점에서 바라봤을 때, 일반적으로는 코드에 문제가 없으면 버그 혹은 보안적인 취약점이 덜 하던 반면에, 블록체인을 이용한 DApp은 그렇지가 않았다. 블록체인 자체의 보안 취약점과 App이 가지는 근본적인 취약점이 합쳐저서 훨씬 불안정한 상태의 기술이라고 느꼈다. 따라서 4차 산업혁명을 이끌어가는 기술들에 대해서 무조건 좋다가 아니라, 한번쯤은 비판적으로 바라봐야한다고 생각한다.","link":"/2022/07/08/About-Block-Chain/"}],"tags":[{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"retrospect","slug":"retrospect","link":"/tags/retrospect/"},{"name":"hadoop, maven","slug":"hadoop-maven","link":"/tags/hadoop-maven/"},{"name":"hadoop, mapreduce","slug":"hadoop-mapreduce","link":"/tags/hadoop-mapreduce/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"AI, TF","slug":"AI-TF","link":"/tags/AI-TF/"},{"name":"ROS, Pixhawk, Jetson","slug":"ROS-Pixhawk-Jetson","link":"/tags/ROS-Pixhawk-Jetson/"},{"name":"BlockChine","slug":"BlockChine","link":"/tags/BlockChine/"},{"name":"AI, paper","slug":"AI-paper","link":"/tags/AI-paper/"}],"categories":[{"name":"hadoop","slug":"hadoop","link":"/categories/hadoop/"},{"name":"maven","slug":"maven","link":"/categories/maven/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"ROS","slug":"ROS","link":"/categories/ROS/"}]}